{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":492536,"sourceType":"datasetVersion","datasetId":230545},{"sourceId":13843319,"sourceType":"datasetVersion","datasetId":8817155},{"sourceId":13850431,"sourceType":"datasetVersion","datasetId":8822526},{"sourceId":13857423,"sourceType":"datasetVersion","datasetId":8827720},{"sourceId":13857514,"sourceType":"datasetVersion","datasetId":8827791}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:47.304091Z","iopub.execute_input":"2025-11-24T14:00:47.304313Z","iopub.status.idle":"2025-11-24T14:00:55.108714Z","shell.execute_reply.started":"2025-11-24T14:00:47.304285Z","shell.execute_reply":"2025-11-24T14:00:55.107876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport scipy.io as sio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.110600Z","iopub.execute_input":"2025-11-24T14:00:55.111192Z","iopub.status.idle":"2025-11-24T14:00:55.329377Z","shell.execute_reply.started":"2025-11-24T14:00:55.111171Z","shell.execute_reply":"2025-11-24T14:00:55.328585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base = \"/kaggle/input/shanghaitech/ShanghaiTech\"\n\npartA_train_img = base + \"/part_A/train_data/images\"\npartA_train_gt  = base + \"/part_A/train_data/ground-truth\"\n\nprint(\"Images:\", len(os.listdir(partA_train_img)))\nprint(\"Ground truth files:\", len(os.listdir(partA_train_gt)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.330208Z","iopub.execute_input":"2025-11-24T14:00:55.330624Z","iopub.status.idle":"2025-11-24T14:00:55.336387Z","shell.execute_reply.started":"2025-11-24T14:00:55.330598Z","shell.execute_reply":"2025-11-24T14:00:55.335801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_paths = sorted(glob.glob(os.path.join(partA_train_img, \"*.jpg\")))\nsample_img_path = img_paths[265]     # first image\nsample_img_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.337131Z","iopub.execute_input":"2025-11-24T14:00:55.337365Z","iopub.status.idle":"2025-11-24T14:00:55.359036Z","shell.execute_reply.started":"2025-11-24T14:00:55.337348Z","shell.execute_reply":"2025-11-24T14:00:55.358298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(sample_img_path).convert('RGB')\nimg_np = np.array(img)\n\nbasename = os.path.basename(sample_img_path).replace(\".jpg\", \"\")\nmat_path = os.path.join(partA_train_gt, \"GT_\" + basename + \".mat\")\nmat = sio.loadmat(mat_path)\n\n# Extract points (x, y)\npoints = mat[\"image_info\"][0][0][0][0][0]   # MATLAB's annoying nested indexing\npoints = np.array(points)\nprint(\"Number of head annotations:\", len(points))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.359733Z","iopub.execute_input":"2025-11-24T14:00:55.360022Z","iopub.status.idle":"2025-11-24T14:00:55.432623Z","shell.execute_reply.started":"2025-11-24T14:00:55.360004Z","shell.execute_reply":"2025-11-24T14:00:55.431838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(sample_img_path).convert('RGB')\nimg_np = np.array(img)\n\nbasename = os.path.basename(sample_img_path).replace(\".jpg\", \"\")\nmat_path = os.path.join(partA_train_gt, \"GT_\" + basename + \".mat\")\nmat = sio.loadmat(mat_path)\n\n# Extract points (x, y)\npoints = mat[\"image_info\"][0][0][0][0][0]   # MATLAB's annoying nested indexing\npoints = np.array(points)\nprint(\"Number of head annotations:\", len(points))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.433407Z","iopub.execute_input":"2025-11-24T14:00:55.433693Z","iopub.status.idle":"2025-11-24T14:00:55.450426Z","shell.execute_reply.started":"2025-11-24T14:00:55.433674Z","shell.execute_reply":"2025-11-24T14:00:55.449757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.ndimage import gaussian_filter\n\nH, W = img_np.shape[:2]\ndensity = np.zeros((H, W), dtype=np.float32)\n\nfor x, y in points:\n    x = min(W - 1, max(0, int(x)))\n    y = min(H - 1, max(0, int(y)))\n    density[y, x] += 1\n\ndensity = gaussian_filter(density, sigma=4)   # sigma=4 is standard for ShanghaiTech\nprint(\"Density sum (crowd count):\", density.sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.452837Z","iopub.execute_input":"2025-11-24T14:00:55.453048Z","iopub.status.idle":"2025-11-24T14:00:55.512863Z","shell.execute_reply.started":"2025-11-24T14:00:55.453031Z","shell.execute_reply":"2025-11-24T14:00:55.512200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(img_np)\nplt.title(\"Original Image\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(density, cmap='jet')\nplt.title(f\"Density Map (count = {density.sum():.1f})\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.513429Z","iopub.execute_input":"2025-11-24T14:00:55.513738Z","iopub.status.idle":"2025-11-24T14:00:55.923181Z","shell.execute_reply.started":"2025-11-24T14:00:55.513721Z","shell.execute_reply":"2025-11-24T14:00:55.922432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = \"/kaggle/working/partA_train_density\"\nos.makedirs(save_dir, exist_ok=True)\n\nnp.save(os.path.join(save_dir, basename + \".npy\"), density)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.923936Z","iopub.execute_input":"2025-11-24T14:00:55.924200Z","iopub.status.idle":"2025-11-24T14:00:55.930885Z","shell.execute_reply.started":"2025-11-24T14:00:55.924181Z","shell.execute_reply":"2025-11-24T14:00:55.930160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crowd_count = density.sum()\nprint(\"Crowd count:\", crowd_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.931642Z","iopub.execute_input":"2025-11-24T14:00:55.931822Z","iopub.status.idle":"2025-11-24T14:00:55.939407Z","shell.execute_reply.started":"2025-11-24T14:00:55.931808Z","shell.execute_reply":"2025-11-24T14:00:55.938676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(img_np)\nplt.title(\"Original Image\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(density, cmap='jet')\nplt.title(f\"Density Map (count = {crowd_count:.1f})\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:55.940241Z","iopub.execute_input":"2025-11-24T14:00:55.940603Z","iopub.status.idle":"2025-11-24T14:00:56.282695Z","shell.execute_reply.started":"2025-11-24T14:00:55.940581Z","shell.execute_reply":"2025-11-24T14:00:56.281882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport scipy.io as sio\nfrom scipy.ndimage import gaussian_filter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:00:56.283427Z","iopub.execute_input":"2025-11-24T14:00:56.283682Z","iopub.status.idle":"2025-11-24T14:01:03.976884Z","shell.execute_reply.started":"2025-11-24T14:00:56.283665Z","shell.execute_reply":"2025-11-24T14:01:03.976269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base = \"/kaggle/input/shanghaitech\"\n\n# auto-detect Part A folders\npartA_train_img = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"train_data\", \"images\"), recursive=True)[0]\npartA_train_gt  = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"train_data\", \"ground*\"), recursive=True)[0]\n\npartA_test_img  = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"test_data\", \"images\"), recursive=True)[0]\npartA_test_gt   = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"test_data\", \"ground*\"), recursive=True)[0]\n\nprint(\"Train img dir :\", partA_train_img)\nprint(\"Train gt dir  :\", partA_train_gt)\nprint(\"Test  img dir :\", partA_test_img)\nprint(\"Test  gt dir  :\", partA_test_gt)\n\nprint(\"Train JPG count:\", len(glob.glob(os.path.join(partA_train_img, \"*.jpg\"))))\nprint(\"Test  JPG count:\", len(glob.glob(os.path.join(partA_test_img, \"*.jpg\"))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:03.977528Z","iopub.execute_input":"2025-11-24T14:01:03.977896Z","iopub.status.idle":"2025-11-24T14:01:06.807848Z","shell.execute_reply.started":"2025-11-24T14:01:03.977878Z","shell.execute_reply":"2025-11-24T14:01:06.807122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_train_density = \"/kaggle/working/partA_train_density\"\nout_test_density  = \"/kaggle/working/partA_test_density\"\nos.makedirs(out_train_density, exist_ok=True)\nos.makedirs(out_test_density, exist_ok=True)\n\ndef generate_density_map(img_shape, points, sigma=4):\n    H, W = img_shape\n    density = np.zeros((H, W), dtype=np.float32)\n    if len(points) == 0:\n        return density\n\n    for x, y in points:\n        x = min(W - 1, max(0, int(x)))\n        y = min(H - 1, max(0, int(y)))\n        density[y, x] += 1\n\n    density = gaussian_filter(density, sigma=sigma)\n    return density\n\ndef process_split(img_dir, gt_dir, out_dir):\n    img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n    print(f\"Processing {len(img_paths)} images in {img_dir}\")\n    for img_path in img_paths:\n        basename = os.path.basename(img_path).replace(\".jpg\", \"\")\n        out_path = os.path.join(out_dir, basename + \".npy\")\n        if os.path.exists(out_path):\n            continue\n\n        img = Image.open(img_path).convert('RGB')\n        img_np = np.array(img)\n        H, W = img_np.shape[:2]\n\n        mat_path = os.path.join(gt_dir, \"GT_\" + basename + \".mat\")\n        mat = sio.loadmat(mat_path)\n\n        # extract (x,y) points\n        points = mat[\"image_info\"][0][0][0][0][0]\n        points = np.array(points)\n\n        density = generate_density_map((H, W), points, sigma=4)\n        np.save(out_path, density)\n    print(\"Done:\", out_dir)\n\nprocess_split(partA_train_img, partA_train_gt, out_train_density)\nprocess_split(partA_test_img,  partA_test_gt,  out_test_density)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:06.808649Z","iopub.execute_input":"2025-11-24T14:01:06.808887Z","iopub.status.idle":"2025-11-24T14:01:26.330953Z","shell.execute_reply.started":"2025-11-24T14:01:06.808868Z","shell.execute_reply":"2025-11-24T14:01:26.330269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ShanghaiTechPartA(Dataset):\n    def __init__(self, img_dir, density_dir, transform=None):\n        self.img_dir = img_dir\n        self.density_dir = density_dir\n        self.transform = transform\n\n        self.img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        basename = os.path.basename(img_path).replace(\".jpg\", \"\")\n\n        # image\n        img = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            img_t = self.transform(img)\n        else:\n            img_t = transforms.ToTensor()(img)\n\n        # density\n        density_path = os.path.join(self.density_dir, basename + \".npy\")\n        density = np.load(density_path).astype(np.float32)\n\n        # IMPORTANT: create new storage tensor (not from_numpy shared)\n        density_t = torch.tensor(density, dtype=torch.float32).unsqueeze(0)  # (1,H,W)\n\n        count = density_t.sum()\n\n        return img_t, density_t, count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:26.331666Z","iopub.execute_input":"2025-11-24T14:01:26.331869Z","iopub.status.idle":"2025-11-24T14:01:26.338087Z","shell.execute_reply.started":"2025-11-24T14:01:26.331853Z","shell.execute_reply":"2025-11-24T14:01:26.337281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        def make_branch(ks):\n            return nn.Sequential(\n                nn.Conv2d(3, 16, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(16, 32, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2),\n                nn.Conv2d(32, 16, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(16, 8, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2),\n            )\n\n        self.branch1 = make_branch(9)\n        self.branch2 = make_branch(7)\n        self.branch3 = make_branch(5)\n\n        # NOTE: no ReLU here – output is raw density\n        self.fuse = nn.Conv2d(8 * 3, 1, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        x_cat = torch.cat((x1, x2, x3), dim=1)\n        out = self.fuse(x_cat)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:26.338935Z","iopub.execute_input":"2025-11-24T14:01:26.339279Z","iopub.status.idle":"2025-11-24T14:01:26.378959Z","shell.execute_reply.started":"2025-11-24T14:01:26.339257Z","shell.execute_reply":"2025-11-24T14:01:26.378207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\ntransform = transforms.Compose([\n    transforms.Resize((480, 640)),   # fixed input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = ShanghaiTechPartA(\n    img_dir=partA_train_img,\n    density_dir=out_train_density,\n    transform=transform\n)\n\ntest_dataset = ShanghaiTechPartA(\n    img_dir=partA_test_img,\n    density_dir=out_test_density,\n    transform=transform\n)\n\nprint(\"Train len:\", len(train_dataset))\nprint(\"Test  len:\", len(test_dataset))\n\n# robust settings: batch_size=1, num_workers=0\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\ntest_loader  = DataLoader(test_dataset,  batch_size=1, shuffle=False, num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:26.379716Z","iopub.execute_input":"2025-11-24T14:01:26.380027Z","iopub.status.idle":"2025-11-24T14:01:26.479839Z","shell.execute_reply.started":"2025-11-24T14:01:26.380010Z","shell.execute_reply":"2025-11-24T14:01:26.479133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, device):\n    model.train()\n    criterion = nn.MSELoss()\n    total_loss = 0.0\n\n    for imgs, densities, _ in loader:\n        imgs = imgs.to(device)\n        densities = densities.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)  # (B,1,h_out,w_out)\n\n        # resize GT density to match output spatial size\n        if outputs.shape[-2:] != densities.shape[-2:]:\n            densities_resized = F.interpolate(\n                densities,\n                size=outputs.shape[-2:],\n                mode='bilinear',\n                align_corners=False\n            )\n        else:\n            densities_resized = densities\n\n        loss = criterion(outputs, densities_resized)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * imgs.size(0)\n\n    return total_loss / len(loader.dataset)\n\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval()\n    mae = 0.0\n    mse = 0.0\n    n = 0\n\n    for imgs, densities, _ in loader:\n        imgs = imgs.to(device)\n        densities = densities.to(device)\n\n        preds = model(imgs)\n\n        # predicted count from output density\n        pred_counts = preds.sum(dim=[1, 2, 3]).cpu()\n\n        # GT count from original density\n        gt_counts   = densities.sum(dim=[1, 2, 3]).cpu()\n\n        diff = pred_counts - gt_counts\n        mae += diff.abs().sum().item()\n        mse += (diff ** 2).sum().item()\n        n += imgs.size(0)\n\n    mae /= n\n    rmse = (mse / n) ** 0.5\n    return mae, rmse\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:26.480586Z","iopub.execute_input":"2025-11-24T14:01:26.480832Z","iopub.status.idle":"2025-11-24T14:01:26.494177Z","shell.execute_reply.started":"2025-11-24T14:01:26.480804Z","shell.execute_reply":"2025-11-24T14:01:26.493597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MCNN().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nnum_epochs = 50  # adjust as you like\nhistory = {\"train_loss\": [], \"mae\": [], \"rmse\": []}\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n    mae, rmse = evaluate(model, test_loader, device)\n\n    history[\"train_loss\"].append(train_loss)\n    history[\"mae\"].append(mae)\n    history[\"rmse\"].append(rmse)\n\n    print(f\"Epoch {epoch:03d}: loss={train_loss:.6f}, MAE={mae:.2f}, RMSE={rmse:.2f}\")\n\ntorch.save(model.state_dict(), \"/kaggle/working/mcnn_partA.pth\")\nprint(\"Model saved to /kaggle/working/mcnn_partA.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:01:26.494894Z","iopub.execute_input":"2025-11-24T14:01:26.495059Z","iopub.status.idle":"2025-11-24T15:04:18.884478Z","shell.execute_reply.started":"2025-11-24T14:01:26.495045Z","shell.execute_reply":"2025-11-24T15:04:18.883709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# one sample from test set\nimgs, dens, _ = next(iter(test_loader))\n\nmodel.eval()\nwith torch.no_grad():\n    preds = model(imgs.to(device))\n\ngt_count   = dens.sum().item()\npred_count = preds.sum().item()\n\nprint(f\"GT count   : {gt_count:.2f}\")\nprint(f\"Pred count : {pred_count:.2f}\")\n\n# prepare for plotting\nimg_np = imgs[0].permute(1, 2, 0).cpu().numpy()\nimg_np = (img_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\nimg_np = img_np.clip(0, 1)\n\ngt_den   = dens[0, 0].cpu().numpy()\npred_den = preds[0, 0].cpu().numpy()\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(img_np)\nplt.title(\"Image\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(gt_den, cmap='jet')\nplt.title(f\"GT density (count={gt_count:.1f})\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(pred_den, cmap='jet')\nplt.title(f\"Pred density (count={pred_count:.1f})\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T15:04:18.885377Z","iopub.execute_input":"2025-11-24T15:04:18.885658Z","execution_failed":"2025-11-24T16:03:51.105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport scipy.io as sio\nfrom scipy.ndimage import gaussian_filter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:04:10.012049Z","iopub.execute_input":"2025-11-24T16:04:10.012304Z","iopub.status.idle":"2025-11-24T16:04:17.869897Z","shell.execute_reply.started":"2025-11-24T16:04:10.012276Z","shell.execute_reply":"2025-11-24T16:04:17.869239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ShanghaiTech base\nbase = \"/kaggle/input/shanghaitech\"\n\n# auto-detect Part_A paths\npartA_train_img = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"train_data\", \"images\"), recursive=True)[0]\npartA_train_gt  = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"train_data\", \"ground*\"), recursive=True)[0]\n\npartA_test_img  = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"test_data\", \"images\"), recursive=True)[0]\npartA_test_gt   = glob.glob(os.path.join(base, \"**\", \"part_A*\", \"test_data\", \"ground*\"), recursive=True)[0]\n\nprint(\"Train img dir :\", partA_train_img)\nprint(\"Train gt dir  :\", partA_train_gt)\nprint(\"Test  img dir :\", partA_test_img)\nprint(\"Test  gt dir  :\", partA_test_gt)\nprint(\"Train JPG count:\", len(glob.glob(os.path.join(partA_train_img, '*.jpg'))))\nprint(\"Test  JPG count :\", len(glob.glob(os.path.join(partA_test_img, '*.jpg'))))\n\n# your uploaded VGG16_bn weights\nvgg_path = \"/kaggle/input/vgg-data/vgg16_bn-6c64b313.pth\"\nassert os.path.exists(vgg_path), \"vgg16_bn-6c64b313.pth not found at /kaggle/input/vgg-data/\"\nprint(\"VGG weights path:\", vgg_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:04:21.014844Z","iopub.execute_input":"2025-11-24T16:04:21.015439Z","iopub.status.idle":"2025-11-24T16:04:34.804103Z","shell.execute_reply.started":"2025-11-24T16:04:21.015413Z","shell.execute_reply":"2025-11-24T16:04:34.803406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_train_density = \"/kaggle/working/partA_train_density\"\nout_test_density  = \"/kaggle/working/partA_test_density\"\nos.makedirs(out_train_density, exist_ok=True)\nos.makedirs(out_test_density, exist_ok=True)\n\ndef generate_density_map(img_shape, points, sigma=4):\n    H, W = img_shape\n    density = np.zeros((H, W), dtype=np.float32)\n    if len(points) == 0:\n        return density\n    for x, y in points:\n        x = min(W - 1, max(0, int(x)))\n        y = min(H - 1, max(0, int(y)))\n        density[y, x] += 1\n    density = gaussian_filter(density, sigma=sigma)\n    return density\n\ndef process_split(img_dir, gt_dir, out_dir):\n    img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n    print(f\"Processing {len(img_paths)} images in {img_dir}\")\n    for img_path in img_paths:\n        basename = os.path.basename(img_path).replace(\".jpg\", \"\")\n        out_path = os.path.join(out_dir, basename + \".npy\")\n        if os.path.exists(out_path):\n            continue\n\n        img = Image.open(img_path).convert('RGB')\n        img_np = np.array(img)\n        H, W = img_np.shape[:2]\n\n        mat_path = os.path.join(gt_dir, \"GT_\" + basename + \".mat\")\n        mat = sio.loadmat(mat_path)\n        points = mat[\"image_info\"][0][0][0][0][0]\n        points = np.array(points)\n\n        density = generate_density_map((H, W), points, sigma=4)\n        np.save(out_path, density)\n    print(\"Done:\", out_dir)\n\nprocess_split(partA_train_img, partA_train_gt, out_train_density)\nprocess_split(partA_test_img,  partA_test_gt,  out_test_density)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:04:34.805277Z","iopub.execute_input":"2025-11-24T16:04:34.805479Z","iopub.status.idle":"2025-11-24T16:04:55.148095Z","shell.execute_reply.started":"2025-11-24T16:04:34.805463Z","shell.execute_reply":"2025-11-24T16:04:55.147397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ShanghaiTechPartA(Dataset):\n    def __init__(self, img_dir, density_dir, transform=None):\n        self.img_dir = img_dir\n        self.density_dir = density_dir\n        self.transform = transform\n        self.img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        basename = os.path.basename(img_path).replace(\".jpg\", \"\")\n\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img_t = self.transform(img)\n        else:\n            img_t = transforms.ToTensor()(img)\n\n        density_path = os.path.join(self.density_dir, basename + \".npy\")\n        density = np.load(density_path).astype(np.float32)\n        density_t = torch.tensor(density, dtype=torch.float32).unsqueeze(0)  # (1,H,W)\n\n        count = density_t.sum()\n        return img_t, density_t, count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:04:55.148801Z","iopub.execute_input":"2025-11-24T16:04:55.148986Z","iopub.status.idle":"2025-11-24T16:04:55.155139Z","shell.execute_reply.started":"2025-11-24T16:04:55.148970Z","shell.execute_reply":"2025-11-24T16:04:55.154306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:04:55.156574Z","iopub.execute_input":"2025-11-24T16:04:55.156776Z","iopub.status.idle":"2025-11-24T16:04:55.266423Z","shell.execute_reply.started":"2025-11-24T16:04:55.156761Z","shell.execute_reply":"2025-11-24T16:04:55.265797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\ntransform = transforms.Compose([\n    transforms.Resize((480, 640)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = ShanghaiTechPartA(\n    img_dir=partA_train_img,\n    density_dir=out_train_density,\n    transform=transform\n)\n\ntest_dataset = ShanghaiTechPartA(\n    img_dir=partA_test_img,\n    density_dir=out_test_density,\n    transform=transform\n)\n\nprint(\"Train len:\", len(train_dataset))\nprint(\"Test  len:\", len(test_dataset))\n\n# batch_size=1 avoids size issues\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\ntest_loader  = DataLoader(test_dataset,  batch_size=1, shuffle=False, num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:04:55.267150Z","iopub.execute_input":"2025-11-24T16:04:55.267374Z","iopub.status.idle":"2025-11-24T16:04:55.357296Z","shell.execute_reply.started":"2025-11-24T16:04:55.267355Z","shell.execute_reply":"2025-11-24T16:04:55.356554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, device):\n    model.train()\n    criterion = nn.MSELoss()\n    total_loss = 0.0\n\n    for imgs, densities, _ in loader:\n        imgs = imgs.to(device)\n        densities = densities.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n\n        # resize GT density to match output spatial size\n        if outputs.shape[-2:] != densities.shape[-2:]:\n            densities_resized = F.interpolate(\n                densities,\n                size=outputs.shape[-2:],\n                mode='bilinear',\n                align_corners=False\n            )\n        else:\n            densities_resized = densities\n\n        loss = criterion(outputs, densities_resized)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * imgs.size(0)\n\n    return total_loss / len(loader.dataset)\n\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval()\n    mae = 0.0\n    mse = 0.0\n    n = 0\n\n    for imgs, densities, _ in loader:\n        imgs = imgs.to(device)\n        densities = densities.to(device)\n\n        preds = model(imgs)\n\n        pred_counts = preds.sum(dim=[1, 2, 3]).cpu()\n        gt_counts   = densities.sum(dim=[1, 2, 3]).cpu()\n\n        diff = pred_counts - gt_counts\n        mae += diff.abs().sum().item()\n        mse += (diff ** 2).sum().item()\n        n += imgs.size(0)\n\n    mae /= n\n    rmse = (mse / n) ** 0.5\n    return mae, rmse\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:14:22.677569Z","iopub.execute_input":"2025-11-24T16:14:22.678274Z","iopub.status.idle":"2025-11-24T16:14:22.685758Z","shell.execute_reply.started":"2025-11-24T16:14:22.678249Z","shell.execute_reply":"2025-11-24T16:14:22.684989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_csr = CSRNet(vgg_weight_path=vgg_path).to(device)\noptimizer_csr = torch.optim.Adam(model_csr.parameters(), lr=1e-5)\n\nnum_epochs = 50   # you can later increase to 50\nhistory_csr = {\"train_loss\": [], \"mae\": [], \"rmse\": []}\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train_one_epoch(model_csr, train_loader, optimizer_csr, device)\n    mae, rmse = evaluate(model_csr, test_loader, device)\n\n    history_csr[\"train_loss\"].append(train_loss)\n    history_csr[\"mae\"].append(mae)\n    history_csr[\"rmse\"].append(rmse)\n\n    print(f\"[CSRNet] Epoch {epoch:03d}: loss={train_loss:.6f}, MAE={mae:.2f}, RMSE={rmse:.2f}\")\n\ntorch.save(model_csr.state_dict(), \"/kaggle/working/csrnet_partA_fromscratch.pth\")\nprint(\"Saved CSRNet to /kaggle/working/csrnet_partA_fromscratch.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:55:30.730983Z","iopub.execute_input":"2025-11-24T16:55:30.731750Z","iopub.status.idle":"2025-11-24T17:56:13.302729Z","shell.execute_reply.started":"2025-11-24T16:55:30.731721Z","shell.execute_reply":"2025-11-24T17:56:13.301878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, models\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:37.830378Z","iopub.execute_input":"2025-11-25T02:00:37.831062Z","iopub.status.idle":"2025-11-25T02:00:37.835421Z","shell.execute_reply.started":"2025-11-25T02:00:37.831029Z","shell.execute_reply":"2025-11-25T02:00:37.834552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# model checkpoints\nMCNN_CKPT   = \"/kaggle/input/csrnet-and-mcnn-models/mcnn_model.pth\"\nCSRNET_CKPT = \"/kaggle/input/csrnet-model/csrnet_partA_finetuned.pth\"\n\n# (optional) VGG16_bn pretrained file (used during training)\nVGG_PTH     = \"/kaggle/input/vgg-data/vgg16_bn-6c64b313.pth\"\n\n# real-world dataset root\nREAL_ROOT   = \"/kaggle/input/real-world\"\nprint(\"Real-world root:\", REAL_ROOT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:38.198226Z","iopub.execute_input":"2025-11-25T02:00:38.198509Z","iopub.status.idle":"2025-11-25T02:00:38.202906Z","shell.execute_reply.started":"2025-11-25T02:00:38.198460Z","shell.execute_reply":"2025-11-25T02:00:38.202114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- MCNN ----------------\n\nclass MCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        def make_branch(ks):\n            return nn.Sequential(\n                nn.Conv2d(3, 16, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(16, 32, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2),\n                nn.Conv2d(32, 16, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(16, 8, kernel_size=ks, padding=ks//2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2),\n            )\n\n        self.branch1 = make_branch(9)\n        self.branch2 = make_branch(7)\n        self.branch3 = make_branch(5)\n        # final 1x1 conv, no ReLU to allow any density\n        self.fuse    = nn.Conv2d(8 * 3, 1, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        x_cat = torch.cat((x1, x2, x3), dim=1)\n        out = self.fuse(x_cat)\n        return out\n\n\n# ---------------- CSRNet ----------------\n\nclass CSRNet(nn.Module):\n    def __init__(self, vgg_weight_path=None):\n        super().__init__()\n\n        # Build VGG16_bn frontend; if vgg_weight_path is given, load weights.\n        vgg = models.vgg16_bn(weights=None)\n        if vgg_weight_path is not None and os.path.exists(vgg_weight_path):\n            state = torch.load(vgg_weight_path, map_location=\"cpu\")\n            vgg.load_state_dict(state)\n        features = list(vgg.features.children())\n\n        # Frontend: conv1_1 to conv4_3 (33 layers)\n        self.frontend = nn.Sequential(*features[:33])\n\n        # Backend: dilated convolutions (CSRNet)\n        self.backend = nn.Sequential(\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n        )\n\n        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self, x):\n        x = self.frontend(x)\n        x = self.backend(x)\n        x = self.output_layer(x)\n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:38.606265Z","iopub.execute_input":"2025-11-25T02:00:38.606714Z","iopub.status.idle":"2025-11-25T02:00:38.619934Z","shell.execute_reply.started":"2025-11-25T02:00:38.606674Z","shell.execute_reply":"2025-11-25T02:00:38.618941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load MCNN weights\nmcnn = MCNN().to(device)\nmcnn_state = torch.load(MCNN_CKPT, map_location=device)\nmcnn.load_state_dict(mcnn_state)\nmcnn.eval()\nprint(\"Loaded MCNN from\", MCNN_CKPT)\n\n# Load CSRNet weights (architecture + VGG init, then trained weights)\ncsrnet = CSRNet(vgg_weight_path=VGG_PTH).to(device)\ncsr_state = torch.load(CSRNET_CKPT, map_location=device)\ncsrnet.load_state_dict(csr_state)\ncsrnet.eval()\nprint(\"Loaded CSRNet from\", CSRNET_CKPT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:38.933031Z","iopub.execute_input":"2025-11-25T02:00:38.933615Z","iopub.status.idle":"2025-11-25T02:00:42.201950Z","shell.execute_reply.started":"2025-11-25T02:00:38.933576Z","shell.execute_reply":"2025-11-25T02:00:42.201194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_infer = transforms.Compose([\n    transforms.Resize((480, 640)),  # same as training\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nimg_paths = []\nfor ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n    img_paths.extend(glob.glob(os.path.join(REAL_ROOT, \"**\", ext), recursive=True))\nimg_paths = sorted(img_paths)\n\nprint(\"Total real-world images found:\", len(img_paths))\nif len(img_paths) == 0:\n    raise RuntimeError(\"No images found under /kaggle/input/real-world/**\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:42.203435Z","iopub.execute_input":"2025-11-25T02:00:42.203738Z","iopub.status.idle":"2025-11-25T02:00:42.347312Z","shell.execute_reply.started":"2025-11-25T02:00:42.203718Z","shell.execute_reply":"2025-11-25T02:00:42.346750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gt_map = {}  # basename -> gt_count\n\ncsv_path = os.path.join(REAL_ROOT, \"counts.csv\")\nif os.path.exists(csv_path):\n    print(\"Found counts.csv at\", csv_path)\n    df_gt = pd.read_csv(csv_path)\n    print(\"counts.csv columns:\", df_gt.columns.tolist())\n    print(df_gt.head())\n\n    # try to guess the filename and gt columns\n    img_col_candidates = [\"image\", \"img\", \"filename\", \"file\", \"name\"]\n    gt_col_candidates  = [\"gt\", \"gt_count\", \"count\", \"people\", \"num_people\"]\n\n    img_col = next((c for c in img_col_candidates if c in df_gt.columns), None)\n    gt_col  = next((c for c in gt_col_candidates  if c in df_gt.columns), None)\n\n    if img_col is None or gt_col is None:\n        print(\"⚠ Could not automatically detect GT columns. \"\n              \"Will skip GT for now.\")\n    else:\n        for _, row in df_gt.iterrows():\n            basename = os.path.basename(str(row[img_col]))\n            gt_map[basename] = float(row[gt_col])\n        print(f\"Loaded GT counts for {len(gt_map)} images from counts.csv.\")\nelse:\n    print(\"No counts.csv found, proceeding without GT.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:42.347985Z","iopub.execute_input":"2025-11-25T02:00:42.348216Z","iopub.status.idle":"2025-11-25T02:00:42.368037Z","shell.execute_reply.started":"2025-11-25T02:00:42.348190Z","shell.execute_reply":"2025-11-25T02:00:42.367527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def safety_label(count, low=50, high=150):\n    \"\"\"\n    Simple safety heuristic:\n      count < low         -> 'SAFE'\n      low <= count < high -> 'WARNING'\n      count >= high       -> 'DANGER'\n    Adjust 'low' and 'high' based on your use-case.\n    \"\"\"\n    if count < low:\n        return \"SAFE\"\n    elif count < high:\n        return \"WARNING\"\n    else:\n        return \"DANGER\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:42.369273Z","iopub.execute_input":"2025-11-25T02:00:42.369494Z","iopub.status.idle":"2025-11-25T02:00:42.373218Z","shell.execute_reply.started":"2025-11-25T02:00:42.369458Z","shell.execute_reply":"2025-11-25T02:00:42.372619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\n\nfor idx, img_path in enumerate(img_paths):\n    img = Image.open(img_path).convert(\"RGB\")\n    img_t = transform_infer(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        den_mcnn   = mcnn(img_t)\n        den_csrnet = csrnet(img_t)\n\n        count_mcnn   = float(den_mcnn.sum().item())\n        count_csrnet = float(den_csrnet.sum().item())\n\n    basename = os.path.basename(img_path)\n    rel      = os.path.relpath(img_path, REAL_ROOT)\n    scene    = rel.split(os.sep)[0]  # top-level folder: beach_1, subway_2, etc.\n\n    gt_count = gt_map.get(basename, np.nan)\n    label    = safety_label(count_csrnet)\n\n    results.append({\n        \"scene\": scene,\n        \"image\": basename,\n        \"full_path\": img_path,\n        \"gt_count\": gt_count,\n        \"mcnn_count\": count_mcnn,\n        \"csrnet_count\": count_csrnet,\n        \"error_mcnn\": (count_mcnn - gt_count) if not np.isnan(gt_count) else np.nan,\n        \"error_csrnet\": (count_csrnet - gt_count) if not np.isnan(gt_count) else np.nan,\n        \"safety_label\": label,\n    })\n\n    # visualize first few examples\n    if idx < 3:\n        img_np = np.array(img)\n        den_np = den_csrnet[0, 0].cpu().numpy()\n\n        plt.figure(figsize=(11,4))\n        plt.subplot(1,2,1)\n        plt.imshow(img_np)\n        plt.title(f\"{scene} | {basename}\")\n        plt.axis(\"off\")\n\n        plt.subplot(1,2,2)\n        plt.imshow(den_np, cmap=\"jet\")\n        plt.title(f\"CSRNet count = {count_csrnet:.1f} ({label})\")\n        plt.axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\nlen(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:00:42.374003Z","iopub.execute_input":"2025-11-25T02:00:42.374254Z","iopub.status.idle":"2025-11-25T02:01:08.147089Z","shell.execute_reply.started":"2025-11-25T02:00:42.374232Z","shell.execute_reply":"2025-11-25T02:01:08.146520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(results)\ncsv_out = \"/kaggle/working/real_world_mcnn_csrnet_results.csv\"\ndf.to_csv(csv_out, index=False)\nprint(\"Saved results CSV to:\", csv_out)\n\ndisplay(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:01:08.147777Z","iopub.execute_input":"2025-11-25T02:01:08.148030Z","iopub.status.idle":"2025-11-25T02:01:08.164105Z","shell.execute_reply.started":"2025-11-25T02:01:08.148011Z","shell.execute_reply":"2025-11-25T02:01:08.163523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if df[\"gt_count\"].notna().any():\n    mask = df[\"gt_count\"].notna()\n    mae_mcnn = (df.loc[mask, \"error_mcnn\"].abs()).mean()\n    mae_csr  = (df.loc[mask, \"error_csrnet\"].abs()).mean()\n\n    rmse_mcnn = np.sqrt((df.loc[mask, \"error_mcnn\"]**2).mean())\n    rmse_csr  = np.sqrt((df.loc[mask, \"error_csrnet\"]**2).mean())\n\n    print(f\"Real-world MAE  (MCNN) : {mae_mcnn:.2f}\")\n    print(f\"Real-world RMSE (MCNN) : {rmse_mcnn:.2f}\")\n    print(f\"Real-world MAE  (CSRNet): {mae_csr:.2f}\")\n    print(f\"Real-world RMSE (CSRNet): {rmse_csr:.2f}\")\n\n    print(\"\\nPer-scene CSRNet MAE:\")\n    display(df.loc[mask].groupby(\"scene\")[\"error_csrnet\"]\n            .apply(lambda x: x.abs().mean()))\nelse:\n    print(\"No GT counts available; skipping MAE/RMSE computation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:01:08.164861Z","iopub.execute_input":"2025-11-25T02:01:08.165148Z","iopub.status.idle":"2025-11-25T02:01:08.181141Z","shell.execute_reply.started":"2025-11-25T02:01:08.165131Z","shell.execute_reply":"2025-11-25T02:01:08.180650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nSafety label distribution (CSRNet-based):\")\ndisplay(df[\"safety_label\"].value_counts())\n\nprint(\"\\nAverage CSRNet count per scene:\")\ndisplay(df.groupby(\"scene\")[\"csrnet_count\"].mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:10:29.491599Z","iopub.execute_input":"2025-11-25T02:10:29.492206Z","iopub.status.idle":"2025-11-25T02:10:29.502255Z","shell.execute_reply.started":"2025-11-25T02:10:29.492175Z","shell.execute_reply":"2025-11-25T02:10:29.501533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if df is not already in memory, load it\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"/kaggle/working/real_world_mcnn_csrnet_results.csv\")\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:10:39.837004Z","iopub.execute_input":"2025-11-25T02:10:39.837720Z","iopub.status.idle":"2025-11-25T02:10:39.852055Z","shell.execute_reply.started":"2025-11-25T02:10:39.837691Z","shell.execute_reply":"2025-11-25T02:10:39.851529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask = df[\"gt_count\"].notna()\n\ngt  = df.loc[mask, \"gt_count\"].values\npred_mcnn  = df.loc[mask, \"mcnn_count\"].values\npred_csr   = df.loc[mask, \"csrnet_count\"].values\n\nplt.figure(figsize=(6,6))\nplt.scatter(gt, pred_mcnn, label=\"MCNN\", alpha=0.6)\nplt.scatter(gt, pred_csr,  label=\"CSRNet\", alpha=0.6, marker=\"x\")\n\nmax_val = max(gt.max(), pred_mcnn.max(), pred_csr.max())\nplt.plot([0, max_val], [0, max_val], linestyle=\"--\")  # y=x reference\n\nplt.xlabel(\"Ground-truth count\")\nplt.ylabel(\"Predicted count\")\nplt.title(\"GT vs Predicted Counts (Real-world dataset)\")\nplt.legend()\nplt.grid(True, linestyle=\":\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:10:41.898168Z","iopub.execute_input":"2025-11-25T02:10:41.898767Z","iopub.status.idle":"2025-11-25T02:10:42.117420Z","shell.execute_reply.started":"2025-11-25T02:10:41.898739Z","shell.execute_reply":"2025-11-25T02:10:42.116731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask = df[\"gt_count\"].notna()\n\n# per-scene MAE (absolute error mean)\nscene_mae_mcnn = df.loc[mask].groupby(\"scene\")[\"error_mcnn\"].apply(lambda x: x.abs().mean())\nscene_mae_csr  = df.loc[mask].groupby(\"scene\")[\"error_csrnet\"].apply(lambda x: x.abs().mean())\n\nscenes = scene_mae_csr.index.tolist()\nx = np.arange(len(scenes))\nwidth = 0.35\n\nplt.figure(figsize=(10,4))\nplt.bar(x - width/2, scene_mae_mcnn.values, width, label=\"MCNN\")\nplt.bar(x + width/2, scene_mae_csr.values,  width, label=\"CSRNet\")\n\nplt.xticks(x, scenes, rotation=45, ha=\"right\")\nplt.ylabel(\"MAE (|pred - GT|)\")\nplt.title(\"Per-scene MAE on Real-world Dataset\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:10:52.038910Z","iopub.execute_input":"2025-11-25T02:10:52.039666Z","iopub.status.idle":"2025-11-25T02:10:52.257585Z","shell.execute_reply.started":"2025-11-25T02:10:52.039639Z","shell.execute_reply":"2025-11-25T02:10:52.256949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask = df[\"gt_count\"].notna()\n\nscene_gt_mean   = df.loc[mask].groupby(\"scene\")[\"gt_count\"].mean()\nscene_pred_mean = df.loc[mask].groupby(\"scene\")[\"csrnet_count\"].mean()\n\nscenes = scene_gt_mean.index.tolist()\nx = np.arange(len(scenes))\nwidth = 0.35\n\nplt.figure(figsize=(10,4))\nplt.bar(x - width/2, scene_gt_mean.values,   width, label=\"GT\")\nplt.bar(x + width/2, scene_pred_mean.values, width, label=\"CSRNet\")\n\nplt.xticks(x, scenes, rotation=45, ha=\"right\")\nplt.ylabel(\"Average count\")\nplt.title(\"Per-scene Average GT vs CSRNet Count\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:11:09.631891Z","iopub.execute_input":"2025-11-25T02:11:09.632447Z","iopub.status.idle":"2025-11-25T02:11:09.864401Z","shell.execute_reply.started":"2025-11-25T02:11:09.632422Z","shell.execute_reply":"2025-11-25T02:11:09.863825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_counts = df[\"safety_label\"].value_counts()\n\nplt.figure(figsize=(4,4))\nplt.bar(label_counts.index, label_counts.values)\nplt.xlabel(\"Safety label\")\nplt.ylabel(\"Number of frames\")\nplt.title(\"Safety Label Distribution (CSRNet-based)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:11:27.144538Z","iopub.execute_input":"2025-11-25T02:11:27.145208Z","iopub.status.idle":"2025-11-25T02:11:27.277133Z","shell.execute_reply.started":"2025-11-25T02:11:27.145170Z","shell.execute_reply":"2025-11-25T02:11:27.276393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask = df[\"gt_count\"].notna()\nerrors_csr = df.loc[mask, \"error_csrnet\"]\n\nplt.figure(figsize=(6,4))\nplt.hist(errors_csr, bins=30)\nplt.xlabel(\"Prediction error (CSRNet: pred - GT)\")\nplt.ylabel(\"Number of frames\")\nplt.title(\"Distribution of CSRNet Prediction Errors\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:11:35.985103Z","iopub.execute_input":"2025-11-25T02:11:35.985692Z","iopub.status.idle":"2025-11-25T02:11:36.166089Z","shell.execute_reply.started":"2025-11-25T02:11:35.985667Z","shell.execute_reply":"2025-11-25T02:11:36.165354Z"}},"outputs":[],"execution_count":null}]}